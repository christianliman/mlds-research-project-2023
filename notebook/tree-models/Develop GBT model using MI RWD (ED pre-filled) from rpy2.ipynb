{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afb75072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import copy\n",
    "import shap\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from scipy import linalg\n",
    "from scipy.special import expit\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from matplotlib import cm\n",
    "from sklearn.base import TransformerMixin, ClassifierMixin\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n",
    "                              RandomForestRegressor, GradientBoostingRegressor)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (roc_auc_score, f1_score, precision_score, recall_score, \n",
    "                             RocCurveDisplay, PrecisionRecallDisplay, \n",
    "                             mean_squared_error)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from pandas.api.types import CategoricalDtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31d480ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modelling functions\n",
    "from ensemble_functions import *\n",
    "from weighting_functions import *\n",
    "from explainer_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28a4617",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "### Imputed using MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e5f99ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "imputed_file = \"imputed_r_sens.pickle\"\n",
    "mimic_dir = \"../../data/mimic-iii/\"\n",
    "Ns = [20000, 10000, 5000, 2000, 1000]\n",
    "\n",
    "# Place to store results\n",
    "results_path = \"../../results/metrics/\"\n",
    "if not os.path.exists(results_path):\n",
    "    os.mkdir(results_path)\n",
    "figures_path = \"../../results/figures/gbt_models_rpy2/\"\n",
    "if not os.path.exists(figures_path):\n",
    "    os.mkdir(figures_path)\n",
    "model_iter_name = \"_rpy2\"\n",
    "\n",
    "# Load imputed dataset\n",
    "with open(mimic_dir + imputed_file, \"rb\") as handle:\n",
    "    mimic_imputed = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f762d85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the categorical variables\n",
    "# Note: as we do not need the columns of missingflag to align, we only modify the data\n",
    "# but do note that this may cause bug in future\n",
    "ohe_ethnicity = OneHotEncoder(drop=\"first\", sparse=False).fit(\n",
    "    mimic_imputed[Ns[0]][\"imp\"][0][[\"ETHNICITY\"]])\n",
    "ohe_gender = OneHotEncoder(drop=\"first\", sparse=False).fit(\n",
    "    mimic_imputed[Ns[0]][\"imp\"][0][[\"GENDER\"]]\n",
    ")\n",
    "ohe_marital = OneHotEncoder(drop=\"first\", sparse=False).fit(\n",
    "    mimic_imputed[Ns[0]][\"imp\"][0][[\"MARITALSTATUS\"]])\n",
    "\n",
    "for n in Ns:\n",
    "    for m, imp in enumerate(mimic_imputed[n][\"imp\"]):\n",
    "        # Transform gender column\n",
    "        gender_temp = ohe_gender.transform(imp[[\"GENDER\"]])\n",
    "        gender_temp = pd.DataFrame(gender_temp, columns=ohe_gender.get_feature_names_out(), \n",
    "                                   index=imp.index)\n",
    "        mimic_imputed[n][\"imp\"][m] = pd.concat(\n",
    "            [mimic_imputed[n][\"imp\"][m], gender_temp], axis=1\n",
    "        ).drop([\"GENDER\"], axis=1)\n",
    "        \n",
    "        # Transform ethnicity column\n",
    "        eth_temp = ohe_ethnicity.transform(imp[[\"ETHNICITY\"]])\n",
    "        eth_temp = pd.DataFrame(eth_temp, columns=ohe_ethnicity.get_feature_names_out(), \n",
    "                                index=imp.index)\n",
    "        mimic_imputed[n][\"imp\"][m] = pd.concat(\n",
    "            [mimic_imputed[n][\"imp\"][m], eth_temp], axis=1\n",
    "        ).drop([\"ETHNICITY\"], axis=1)\n",
    "        \n",
    "        # Transform marital status column\n",
    "        marital_temp = ohe_marital.transform(imp[[\"MARITALSTATUS\"]])\n",
    "        marital_temp = pd.DataFrame(marital_temp, columns=ohe_marital.get_feature_names_out(), \n",
    "                                    index=imp.index)\n",
    "        mimic_imputed[n][\"imp\"][m] = pd.concat(\n",
    "            [mimic_imputed[n][\"imp\"][m], marital_temp], axis=1\n",
    "        ).drop([\"MARITALSTATUS\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9387c59f",
   "metadata": {},
   "source": [
    "### Complete case data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48d3db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load complete case data\n",
    "mimic_cc = {}\n",
    "for n in Ns:\n",
    "    mimic_cc[n] = pd.read_csv(mimic_dir + \"complete_case_{}_sens.csv\".format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2945401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the categorical variables\n",
    "for n in Ns:\n",
    "    # Transform gender column\n",
    "    gender_temp = ohe_gender.transform(mimic_cc[n][[\"GENDER\"]])\n",
    "    gender_temp = pd.DataFrame(gender_temp, columns=ohe_gender.get_feature_names_out(), \n",
    "                               index=mimic_cc[n].index)\n",
    "    mimic_cc[n] = pd.concat(\n",
    "        [mimic_cc[n], gender_temp], axis=1\n",
    "    ).drop([\"GENDER\"], axis=1)\n",
    "\n",
    "    # Transform ethnicity column\n",
    "    eth_temp = ohe_ethnicity.transform(mimic_cc[n][[\"ETHNICITY\"]])\n",
    "    eth_temp = pd.DataFrame(eth_temp, columns=ohe_ethnicity.get_feature_names_out(), \n",
    "                            index=mimic_cc[n].index)\n",
    "    mimic_cc[n] = pd.concat(\n",
    "        [mimic_cc[n], eth_temp], axis=1\n",
    "    ).drop([\"ETHNICITY\"], axis=1)\n",
    "\n",
    "    # Transform marital status column\n",
    "    marital_temp = ohe_marital.transform(mimic_cc[n][[\"MARITALSTATUS\"]])\n",
    "    marital_temp = pd.DataFrame(marital_temp, columns=ohe_marital.get_feature_names_out(), \n",
    "                                index=mimic_cc[n].index)\n",
    "    mimic_cc[n] = pd.concat(\n",
    "        [mimic_cc[n], marital_temp], axis=1\n",
    "    ).drop([\"MARITALSTATUS\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb125106",
   "metadata": {},
   "source": [
    "## Model development and assessment\n",
    "\n",
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf839c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine covariates and outcome variables\n",
    "yvar = \"READM90D\"\n",
    "Xvars = mimic_cc[Ns[0]].drop(\"READM90D\", axis=1).columns.values\n",
    "\n",
    "# Selected features for model explanation\n",
    "selected_expl = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deb8a411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 20000, number of readmissions = 1595 / 20000\n",
      "Dataset 10000, number of readmissions = 785 / 10000\n",
      "Dataset 5000, number of readmissions = 391 / 5000\n",
      "Dataset 2000, number of readmissions = 155 / 2000\n",
      "Dataset 1000, number of readmissions = 75 / 1000\n"
     ]
    }
   ],
   "source": [
    "# Get readmission % in each dataset\n",
    "for n in Ns:\n",
    "    print(\"Dataset {}, number of readmissions = {} / {}\".format(\n",
    "        n, mimic_imputed[n][\"imp\"][0][yvar].sum(), n\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6428ffcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 20000 (CC), number of readmissions = 1413 / 16113\n",
      "Dataset 10000 (CC), number of readmissions = 696 / 8033\n",
      "Dataset 5000 (CC), number of readmissions = 346 / 4035\n",
      "Dataset 2000 (CC), number of readmissions = 133 / 1614\n",
      "Dataset 1000 (CC), number of readmissions = 65 / 809\n"
     ]
    }
   ],
   "source": [
    "# Get readmission % in each dataset\n",
    "for n in Ns:\n",
    "    print(\"Dataset {} (CC), number of readmissions = {} / {}\".format(\n",
    "        n, mimic_cc[n][yvar].sum(), mimic_cc[n].shape[0]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa318acb",
   "metadata": {},
   "source": [
    "### General model setup (GBT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12f56bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation folds\n",
    "n_splits = 5\n",
    "\n",
    "# Set random seed\n",
    "SEED = 2023\n",
    "\n",
    "# Probability cutoff to compute metrics such as F1 score\n",
    "pred_cutoff = np.linspace(0.1, 1, 10)\n",
    "\n",
    "## For now, we are using the default setup from sklearn\n",
    "basemdl = GradientBoostingClassifier()\n",
    "basemdlname = \"gbt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96e12e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder tables to store performance metrics (AUROC and F1)\n",
    "mimic_auroc = pd.DataFrame(np.zeros((len(Ns), 3)), index=Ns, \n",
    "                            columns=[\"CC\", \"Ensemble\", \"Weighting\"])\n",
    "mimic_f1 = {}\n",
    "for c in [\"CC\", \"Ensemble\", \"Weighting\"]:\n",
    "    mimic_f1[c] = pd.DataFrame(np.zeros((len(Ns), len(pred_cutoff))), \n",
    "                               index=Ns, columns=pred_cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b4f334",
   "metadata": {},
   "source": [
    "### Complete case data\n",
    "\n",
    "#### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29868315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [00:10<00:00,  2.16s/it]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all versions of the data\n",
    "for n in tqdm(Ns):\n",
    "    # Separate indep and outcome variables\n",
    "    X = mimic_cc[n][Xvars]\n",
    "    y = mimic_cc[n][yvar]\n",
    "    \n",
    "    # Initialise k-fold object\n",
    "    kf = KFold(n_splits=n_splits, random_state=SEED, shuffle=True)\n",
    "    \n",
    "    # Placeholder for predictions to calculate performance metrics\n",
    "    preds = []\n",
    "    \n",
    "    # Set random seed for np.random\n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    # Iterate over the folds\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        # Get train and test data\n",
    "        X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "        X_test, y_test = X.iloc[test_index], y.iloc[test_index]\n",
    "        \n",
    "        # Due to low % of readmissions, need to do resampling of minority class\n",
    "        train_index_pos = y_train[y_train == 1].index\n",
    "        train_index_neg = y_train[y_train == 0].index\n",
    "        train_index_pos_up = np.random.choice(\n",
    "            train_index_pos, size=len(train_index_neg), replace=True\n",
    "        )\n",
    "        train_index_up = np.concatenate((train_index_pos_up, train_index_neg), axis=0)\n",
    "        X_train_up = X_train.loc[train_index_up]\n",
    "        y_train_up = y_train.loc[train_index_up]\n",
    "        \n",
    "        # Train classifier on training data\n",
    "        rf = copy.deepcopy(basemdl).fit(X_train_up, y_train_up)\n",
    "        \n",
    "        # Predict on test data and store predictions\n",
    "        pred_ = rf.predict_proba(X_test)[:, 1]\n",
    "        preds.append(pd.DataFrame({\"true\": y_test, \"pred\": pred_}))\n",
    "    \n",
    "    # Aggregate predictions, compute AUROC and F1 score\n",
    "    preds = pd.concat(preds)\n",
    "    for p in pred_cutoff:\n",
    "        preds[\"pred_labels\"] = preds[\"pred\"] > p\n",
    "        mimic_f1[\"CC\"].loc[n, p] = f1_score(preds[\"true\"], preds[\"pred_labels\"])\n",
    "    mimic_auroc.loc[n, \"CC\"] = roc_auc_score(preds[\"true\"], preds[\"pred\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31c5622",
   "metadata": {},
   "source": [
    "#### Full model training and explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2b2c8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exact explainer: 16114it [09:42, 27.31it/s]                                     \n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "Exact explainer: 8034it [04:20, 29.59it/s]                                      \n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "Exact explainer: 4036it [02:13, 27.88it/s]                                      \n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "Exact explainer: 1615it [00:50, 25.43it/s]                                      \n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "Exact explainer: 810it [00:26, 18.47it/s]                                       \n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n"
     ]
    }
   ],
   "source": [
    "%matplotlib agg\n",
    "# Iterate over all versions of the data\n",
    "for n in Ns:\n",
    "    # Separate indep and outcome variables\n",
    "    X = mimic_cc[n][Xvars]\n",
    "    y = mimic_cc[n][yvar]\n",
    "    \n",
    "    # Due to low % of readmissions, need to do resampling of minority class\n",
    "    index_pos = y[y == 1].index\n",
    "    index_neg = y[y == 0].index\n",
    "    index_pos_up = np.random.choice(\n",
    "        index_pos, size=len(index_neg), replace=True\n",
    "    )\n",
    "    index_up = np.concatenate((index_pos_up, index_neg), axis=0)\n",
    "    X_up = X.loc[index_up]\n",
    "    y_up = y.loc[index_up]\n",
    "    \n",
    "    # Train RF classifier\n",
    "    rf = copy.deepcopy(basemdl).fit(X_up, y_up)\n",
    "    \n",
    "    # Create output path for model explanation\n",
    "    expl_path = figures_path + \"mimic_sens_cc_{}/\".format(n)\n",
    "    if not os.path.exists(expl_path):\n",
    "        os.mkdir(expl_path)\n",
    "    \n",
    "    # Model explanation\n",
    "    background_data = shap.maskers.Independent(X, max_samples=100)\n",
    "    pred_fn = lambda x: rf.predict_proba(x)[:, 1]\n",
    "    expl = shap.Explainer(pred_fn, background_data, link=shap.links.logit)\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    shapvals = expl(X)\n",
    "    \n",
    "    # Create SHAP dependence plots for selected features\n",
    "    for c in selected_expl:\n",
    "        f1, ax1 = plt.subplots(figsize=(5, 5), ncols=1, nrows=1)\n",
    "        # shap.dependence_plot(c, shap_values=shapvals, features=X, show=False, ax=ax1)\n",
    "        shap.plots.scatter(shapvals[:, c], show=False, ax=ax1)\n",
    "        f1.tight_layout()\n",
    "        f1.savefig(expl_path + \"dependence_{}.pdf\".format(c))\n",
    "    \n",
    "    # Create beeswarm plot\n",
    "    plt.clf()\n",
    "    _ = shap.plots.beeswarm(shapvals, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(expl_path + \"beeswarm.pdf\")\n",
    "\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd5fe2",
   "metadata": {},
   "source": [
    "### Ensemble approach\n",
    "\n",
    "#### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "266966f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [03:48<00:00, 45.65s/it]\n"
     ]
    }
   ],
   "source": [
    "# Placeholder for all preds just in case things go wrong\n",
    "all_preds = []\n",
    "\n",
    "# Iterate over all versions of the data\n",
    "for n in tqdm(Ns):\n",
    "    # Prepare ensemble data\n",
    "    X, y = PrepareEnsembleData(mimic_imputed[n], yvar, covars=Xvars)\n",
    "    \n",
    "    # Construct base model\n",
    "    # basemdl = RandomForestClassifier()\n",
    "    \n",
    "    # Run K-fold CV\n",
    "    metrics, preds = KFoldEnsemble(n_splits, X, y, \n",
    "                                   mimic_imputed[n][\"missingflag\"].any(axis=1), \n",
    "                                   basemdl, classifier=True, random_state=SEED, \n",
    "                                   resample=True, pred_cutoff=pred_cutoff)\n",
    "    \n",
    "    # Store metrics\n",
    "    all_preds.append(preds)\n",
    "    mimic_auroc.loc[n, \"Ensemble\"] = metrics[\"AUROC\"]\n",
    "    mimic_f1[\"Ensemble\"].loc[n] = metrics[\"F1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583d018d",
   "metadata": {},
   "source": [
    "#### Full model training and explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "560772e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exact explainer: 16114it [2:34:07,  1.74it/s]                                   \n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "Exact explainer: 8034it [1:16:26,  1.75it/s]                                    \n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "Exact explainer: 4036it [32:37,  2.05it/s]                                      \n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "Exact explainer: 1615it [12:18,  2.16it/s]                                      \n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "Exact explainer: 810it [06:15,  2.09it/s]                                       \n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n"
     ]
    }
   ],
   "source": [
    "%matplotlib agg\n",
    "# Iterate over all versions of the data\n",
    "for n in Ns:\n",
    "    # Prepare ensemble data\n",
    "    X, y = PrepareEnsembleData(mimic_imputed[n], yvar, covars=Xvars)\n",
    "    \n",
    "    # Due to low % of readmissions, need to do resampling of minority class\n",
    "    # only for model training\n",
    "    index_pos = y[0][y[0] == 1].index\n",
    "    index_neg = y[0][y[0] == 0].index\n",
    "    index_pos_up = np.random.choice(\n",
    "        index_pos, size=len(index_neg), replace=True\n",
    "    )\n",
    "    index_up = np.concatenate((index_pos_up, index_neg), axis=0)\n",
    "    X_up, y_up = {}, {}\n",
    "    for j in range(len(X)):\n",
    "        X_up[j] = X[j].loc[index_up]\n",
    "        y_up[j] = y[j].loc[index_up]\n",
    "    \n",
    "    # Construct ensemble model\n",
    "    # basemdl = RandomForestClassifier()\n",
    "    ensemblerf = EnsembleClassifier(basemdl).fit(X_up, y_up)\n",
    "    \n",
    "    # Create output path for model explanation\n",
    "    expl_path = figures_path + \"mimic_sens_ensemble_{}/\".format(n)\n",
    "    if not os.path.exists(expl_path):\n",
    "        os.mkdir(expl_path)\n",
    "    \n",
    "    # Get complete case data for model explanation\n",
    "    mflag = mimic_imputed[n][\"missingflag\"].any(axis=1)\n",
    "    Xobs = X[0][Xvars][~mflag]\n",
    "    \n",
    "    # Model explanation\n",
    "    background_data = shap.maskers.Independent(Xobs, max_samples=100)\n",
    "    pred_fn = lambda x: ensemblerf.predict_proba(x)[:, 1]\n",
    "    expl = shap.Explainer(pred_fn, background_data, link=shap.links.logit)\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    shapvals = expl(Xobs)\n",
    "    \n",
    "    # Create SHAP dependence plots for selected features\n",
    "    for c in selected_expl:\n",
    "        f1, ax1 = plt.subplots(figsize=(5, 5), ncols=1, nrows=1)\n",
    "        # shap.dependence_plot(c, shap_values=shapvals, features=Xobs, show=False, ax=ax1)\n",
    "        shap.plots.scatter(shapvals[:, c], show=False, ax=ax1)\n",
    "        f1.tight_layout()\n",
    "        f1.savefig(expl_path + \"dependence_{}.pdf\".format(c))\n",
    "    \n",
    "    # Create beeswarm plot\n",
    "    plt.clf()\n",
    "    _ = shap.plots.beeswarm(shapvals, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(expl_path + \"beeswarm.pdf\")\n",
    "\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf31dce",
   "metadata": {},
   "source": [
    "### Weighting approach\n",
    "\n",
    "#### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c3f5258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [00:52<00:00, 10.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# Placeholder for all preds just in case things go wrong\n",
    "all_preds = []\n",
    "\n",
    "# Iterate over all versions of the data\n",
    "for n in tqdm(Ns):\n",
    "    # Construct base model\n",
    "    # basemdl = RandomForestClassifier()\n",
    "    \n",
    "    # Run K-fold CV\n",
    "    metrics, preds = KFoldWeighted(n_splits, mimic_imputed[n], yvar, \n",
    "                                   basemdl, classifier=True, random_state=SEED, \n",
    "                                   covars=Xvars, pred_cutoff=pred_cutoff, resample=True)\n",
    "    \n",
    "    # Store metrics\n",
    "    all_preds.append(preds)\n",
    "    mimic_auroc.loc[n, \"Weighting\"] = metrics[\"AUROC\"]\n",
    "    mimic_f1[\"Weighting\"].loc[n] = metrics[\"F1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc9b18",
   "metadata": {},
   "source": [
    "#### Full model training and explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec8c31b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exact explainer: 16114it [08:12, 32.05it/s]                                     \n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "Exact explainer: 8034it [04:07, 31.06it/s]                                      \n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "Exact explainer: 4036it [01:59, 30.99it/s]                                      \n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "Exact explainer: 1615it [00:46, 27.48it/s]                                      \n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "Exact explainer: 810it [00:23, 19.77it/s]                                       \n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n"
     ]
    }
   ],
   "source": [
    "%matplotlib agg\n",
    "# Iterate over all versions of the data\n",
    "for n in Ns:\n",
    "    # Prepare weighted data\n",
    "    X, y, w = PrepareWeightedData(mimic_imputed[n], yvar, covars=Xvars)\n",
    "    \n",
    "    # Due to low % of readmissions, need to do resampling of minority class\n",
    "    # only for model training\n",
    "    index_pos = y[y == 1].index\n",
    "    index_neg = y[y == 0].index\n",
    "    index_pos_up = np.random.choice(\n",
    "        index_pos, size=len(index_neg), replace=True\n",
    "    )\n",
    "    index_up = np.concatenate((index_pos_up, index_neg), axis=0)\n",
    "    X_up = X.loc[index_up]\n",
    "    w_up = pd.Series(w, index=y.index).loc[index_up].values\n",
    "    y_up = y.loc[index_up]\n",
    "    \n",
    "    # Construct weighted model\n",
    "    weightedrf = copy.deepcopy(basemdl).fit(X_up, y_up, sample_weight=w_up)\n",
    "    \n",
    "    # Create output path for model explanation\n",
    "    expl_path = figures_path + \"mimic_sens_weighting_{}/\".format(n)\n",
    "    if not os.path.exists(expl_path):\n",
    "        os.mkdir(expl_path)\n",
    "    \n",
    "    # Get complete case data for model explanation\n",
    "    mflag = mimic_imputed[n][\"missingflag\"].any(axis=1)\n",
    "    Xobs = mimic_imputed[n][\"imp\"][0][Xvars][~mflag]\n",
    "    \n",
    "    # Model explanation\n",
    "    background_data = shap.maskers.Independent(Xobs, max_samples=100)\n",
    "    pred_fn = lambda x: weightedrf.predict_proba(x)[:, 1]\n",
    "    expl = shap.Explainer(pred_fn, background_data, link=shap.links.logit)\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    shapvals = expl(Xobs)\n",
    "    \n",
    "    # Create SHAP dependence plots for selected features\n",
    "    for c in selected_expl:\n",
    "        f1, ax1 = plt.subplots(figsize=(5, 5), ncols=1, nrows=1)\n",
    "        # shap.dependence_plot(c, shap_values=shapvals, features=Xobs, show=False, ax=ax1)\n",
    "        shap.plots.scatter(shapvals[:, c], show=False, ax=ax1)\n",
    "        f1.tight_layout()\n",
    "        f1.savefig(expl_path + \"dependence_{}.pdf\".format(c))\n",
    "    \n",
    "    # Create beeswarm plot\n",
    "    plt.clf()\n",
    "    _ = shap.plots.beeswarm(shapvals, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(expl_path + \"beeswarm.pdf\")\n",
    "\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa268bd4",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1a8ff06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             CC  Ensemble  Weighting\n",
      "20000  0.606524  0.610624   0.608344\n",
      "10000  0.587745  0.594567   0.590841\n",
      "5000   0.555530  0.546955   0.555876\n",
      "2000   0.550413  0.546783   0.537368\n",
      "1000   0.504094  0.492380   0.508333\n"
     ]
    }
   ],
   "source": [
    "# Print all AUROCs\n",
    "print(mimic_auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e88c08c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             CC  Ensemble  Weighting\n",
      "20000  0.161988  0.161709   0.161542\n",
      "10000  0.159264  0.160064   0.159816\n",
      "5000   0.157433  0.157858   0.157350\n",
      "2000   0.160407  0.158151   0.153939\n",
      "1000   0.133903  0.149606   0.149292\n"
     ]
    }
   ],
   "source": [
    "# Print all F1 scores (cutoff 0.1)\n",
    "print(pd.concat(\n",
    "    [mimic_f1[\"CC\"][[0.1]].rename(columns={0.1: \"CC\"}), \n",
    "     mimic_f1[\"Ensemble\"][[0.1]].rename(columns={0.1: \"Ensemble\"}), \n",
    "     mimic_f1[\"Weighting\"][[0.1]].rename(columns={0.1: \"Weighting\"})],\n",
    "    axis=1\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "410bfe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all metrics in CSV\n",
    "mimic_auroc.to_csv(results_path + \"mimic_sens_auroc_{}{}.csv\".format(basemdlname, model_iter_name))\n",
    "#mimic_f1.to_csv(results_path + \"mimic_f1_{}{}.csv\".format(basemdlname, model_iter_name))\n",
    "for c in [\"CC\", \"Ensemble\", \"Weighting\"]:\n",
    "    mimic_f1[c].to_csv(results_path + \"mimic_sens_f1_{}_{}{}.csv\".format(\n",
    "        c, basemdlname, model_iter_name\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
